{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f74bef7",
   "metadata": {},
   "source": [
    "This notebook has for goal to preprocess the Wikidata dataset. \n",
    "\n",
    "# Description of the dataset\n",
    "\n",
    "The wikidata dataset is constituted of 10 smaller subset, each containing names (person_label), date of births (dobs), countries (countries) and continent (continents) of 500 person from a given continent and gender. It also contain the Wikidata identifier for those persons (person). \n",
    "\n",
    "The continent included are : \n",
    "- Africa\n",
    "- Asia\n",
    "- Europe\n",
    "- North America\n",
    "- South America\n",
    "\n",
    "The gender included are : \n",
    "- Male\n",
    "- Female\n",
    "Those two categories include anyone tagged either male, female, cisgender male or cisgender female. \n",
    "\n",
    "When fetching data from Wikidata, the date of birth was capped between 1900 and 2025 to prevent cases where someone's date of birth was not filled. \n",
    "\n",
    "# Preprocessing tasks\n",
    "\n",
    "The current 10 subset will need to be check for duplicates. Some of the people included have multiple countries and continents of citizenship, meaning they might have shown up multiple time on different queries. The check for duplication will be made on the Wikidata identifier. \n",
    "\n",
    "Once checked for duplicates, the dataset will need to be manually treated to identify the first, middle and last name(s) of the people included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dde10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "515ff463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 11 files.\n",
      "Big dataset shape: (9881, 7)\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"../../data/rawData/\")\n",
    "\n",
    "csv_files = list(data_dir.glob(\"*.csv\"))\n",
    "\n",
    "dfs = [pd.read_csv(f) for f in csv_files]\n",
    "\n",
    "big_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Combined {len(csv_files)} files.\")\n",
    "print(f\"Big dataset shape: {big_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6191a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = big_df.drop_duplicates(subset=['person'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a235b832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac6f452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: (9881, 7)\n",
      "After cleaning : (4881, 7)\n",
      "Removed 5000 duplicates (50.60% of rows).\n"
     ]
    }
   ],
   "source": [
    "print(\"Before cleaning:\", big_df.shape)\n",
    "print(\"After cleaning :\", clean_df.shape)\n",
    "\n",
    "duplicate_count = big_df.shape[0] - clean_df.shape[0]\n",
    "duplicate_ratio = duplicate_count / big_df.shape[0] * 100\n",
    "print(f\"Removed {duplicate_count} duplicates ({duplicate_ratio:.2f}% of rows).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7df8bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique persons: 4881\n",
      "Rows in clean_df: 4881\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique persons:\", clean_df['person'].nunique())\n",
    "print(\"Rows in clean_df:\", clean_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2a56054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of missing values per column:\n",
      "Unnamed: 0      0.0\n",
      "person          0.0\n",
      "person_label    0.0\n",
      "genders         0.0\n",
      "dobs            0.0\n",
      "countries       0.0\n",
      "continents      0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing_summary = clean_df.isnull().mean().sort_values(ascending=False)\n",
    "print(\"Fraction of missing values per column:\")\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c28ab42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continents     countries  person_label            \n",
      "Europe         Poland     Andrzej Kowalski            5\n",
      "               Hungary    Tibor Flórián               2\n",
      "               Italy      Lorenzo Casini              2\n",
      "South America  Brazil     Fabio Ribeiro               2\n",
      "               Venezuela  José Urriola                1\n",
      "                          Jhonnatan Medina-Álvarez    1\n",
      "Africa         Algeria    Isma Kaddouri               1\n",
      "South America  Venezuela  Victor Luces                1\n",
      "Africa         Algeria    Abdel Medioub               1\n",
      "                          Asma Guesmi                 1\n",
      "                          Fatima Hellilou             1\n",
      "                          Khaled Benaissa             1\n",
      "                          Leila Beratto               1\n",
      "                          Mohamed Esseghir            1\n",
      "                          Mohamed Zerguini            1\n",
      "                          Myriam Belkiri              1\n",
      "                          Ratiba Derfoul              1\n",
      "                          Selma Hellal                1\n",
      "                          Sofiane Benfatah            1\n",
      "                          Soraya Mellili              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(clean_df[['continents', 'countries', 'person_label']].value_counts(normalize=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f6b22",
   "metadata": {},
   "source": [
    "In this part, we'll explore how much does the names needs curations. \n",
    "\n",
    "Curation, in this context, is defined as needing, in any order : \n",
    "- To separate first, middle and last names\n",
    "- To deal with any potentially problematic character such as joined name (-), names with apostrophes ('), and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3657b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 4881\n",
      "Potential middle names: 882\n",
      "Percentage: 18.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brief\\AppData\\Local\\Temp\\ipykernel_27292\\3307653342.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['hasMiddleName'] = middle_name_mask\n"
     ]
    }
   ],
   "source": [
    "middle_name_mask = clean_df['person_label'].str.count(\" \") >= 2\n",
    "middle_name_count = middle_name_mask.sum()\n",
    "\n",
    "print(f\"Total rows: {clean_df.shape[0]}\")\n",
    "print(f\"Potential middle names: {middle_name_count}\")\n",
    "print(f\"Percentage: {middle_name_count / clean_df.shape[0] * 100:.2f}%\")\n",
    "\n",
    "clean_df['hasMiddleName'] = middle_name_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c26876b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 4881\n",
      "Potential middle names: 55\n",
      "Percentage: 1.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brief\\AppData\\Local\\Temp\\ipykernel_27292\\2899771360.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['hasNoLastName'] = no_last_name_mask\n"
     ]
    }
   ],
   "source": [
    "no_last_name_mask = clean_df['person_label'].str.count(\" \") == 0\n",
    "no_last_name_count = no_last_name_mask.sum()\n",
    "\n",
    "print(f\"Total rows: {clean_df.shape[0]}\")\n",
    "print(f\"Potential middle names: {no_last_name_count}\")\n",
    "print(f\"Percentage: {no_last_name_count / clean_df.shape[0] * 100:.2f}%\")\n",
    "\n",
    "clean_df['hasNoLastName'] = no_last_name_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38aa09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 4881, 'difficult_count': np.int64(1104), 'difficult_pct': np.float64(22.618315918869083), 'by_reason': {'hyphen': 219, 'apostrophe': 32, 'punctuation': 145, 'digit': 1, 'initials': 131, 'non_ascii': 763, 'whitespace': 4}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brief\\AppData\\Local\\Temp\\ipykernel_27292\\1114954087.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df[\"person_label_norm\"] = clean_df[\"person_label\"].map(normalize_name)\n",
      "C:\\Users\\brief\\AppData\\Local\\Temp\\ipykernel_27292\\1114954087.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  initial_mask      = s_norm.str.contains(r\"(^|\\s)[A-Za-z]\\.\", na=False)  # initials like \"J.\"\n",
      "C:\\Users\\brief\\AppData\\Local\\Temp\\ipykernel_27292\\1114954087.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df[\"hasDifficultName\"] = difficult_mask\n",
      "C:\\Users\\brief\\AppData\\Local\\Temp\\ipykernel_27292\\1114954087.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df[\"difficult_reason\"] = [reason_row(i) for i in range(len(clean_df))]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# 1) Keep the original text; add a normalized helper column for consistent tests\n",
    "def normalize_name(s):\n",
    "    if pd.isna(s):\n",
    "        return s\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))  # unify apostrophes/spaces, etc.\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)                # collapse internal whitespace\n",
    "    return s\n",
    "\n",
    "clean_df[\"person_label_norm\"] = clean_df[\"person_label\"].map(normalize_name)\n",
    "\n",
    "# 2) Build masks (vectorized)\n",
    "s_raw  = clean_df[\"person_label\"].astype(str)\n",
    "s_norm = clean_df[\"person_label_norm\"].astype(str)\n",
    "\n",
    "hyphen_mask       = s_norm.str.contains(r\"-\", na=False)\n",
    "apostrophe_mask   = s_norm.str.contains(r\"['\\u2019\\u02BC]\", na=False)  # ', ’, ʼ\n",
    "punct_mask        = s_norm.str.contains(r\"[.,/&(){}\\[\\]<>@#?!$%^*_=+\\\\]\", na=False)\n",
    "digit_mask        = s_norm.str.contains(r\"\\d\", na=False)\n",
    "initial_mask      = s_norm.str.contains(r\"(^|\\s)[A-Za-z]\\.\", na=False)  # initials like \"J.\"\n",
    "# Non-ASCII: after NFKC, compare ASCII-stripped version to itself\n",
    "non_ascii_mask    = ~s_norm.map(lambda x: x.isascii())\n",
    "\n",
    "# Whitespace issues measured on the raw (pre-normalization) text\n",
    "whitespace_mask   = s_raw.str.contains(\n",
    "    r\"^\\s|[\\u00A0\\u2007\\u202F]|\\s{2,}|\\s$\", na=False\n",
    ")\n",
    "\n",
    "# 3) Combine into a single \"hasDifficultName\" flag\n",
    "difficult_mask = (\n",
    "    hyphen_mask\n",
    "    | apostrophe_mask\n",
    "    | punct_mask\n",
    "    | digit_mask\n",
    "    | initial_mask\n",
    "    | non_ascii_mask\n",
    "    | whitespace_mask\n",
    ")\n",
    "\n",
    "clean_df[\"hasDifficultName\"] = difficult_mask\n",
    "\n",
    "# 4) (Optional) Keep a reason code for explainability/auditing\n",
    "def reason_row(i):\n",
    "    reasons = []\n",
    "    if hyphen_mask.iat[i]:      reasons.append(\"hyphen\")\n",
    "    if apostrophe_mask.iat[i]:  reasons.append(\"apostrophe\")\n",
    "    if punct_mask.iat[i]:       reasons.append(\"punctuation\")\n",
    "    if digit_mask.iat[i]:       reasons.append(\"digit\")\n",
    "    if initial_mask.iat[i]:     reasons.append(\"initials\")\n",
    "    if non_ascii_mask.iat[i]:   reasons.append(\"non_ascii\")\n",
    "    if whitespace_mask.iat[i]:  reasons.append(\"whitespace\")\n",
    "    return \"|\".join(reasons)\n",
    "\n",
    "clean_df[\"difficult_reason\"] = [reason_row(i) for i in range(len(clean_df))]\n",
    "\n",
    "# 5) (Optional) Quick summary for your report\n",
    "summary = {\n",
    "    \"total\": len(clean_df),\n",
    "    \"difficult_count\": difficult_mask.sum(),\n",
    "    \"difficult_pct\": 100 * difficult_mask.mean(),\n",
    "    \"by_reason\": {\n",
    "        \"hyphen\": int(hyphen_mask.sum()),\n",
    "        \"apostrophe\": int(apostrophe_mask.sum()),\n",
    "        \"punctuation\": int(punct_mask.sum()),\n",
    "        \"digit\": int(digit_mask.sum()),\n",
    "        \"initials\": int(initial_mask.sum()),\n",
    "        \"non_ascii\": int(non_ascii_mask.sum()),\n",
    "        \"whitespace\": int(whitespace_mask.sum()),\n",
    "    },\n",
    "}\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ffeeef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(\"../../data/rawData/all.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "name2gender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
